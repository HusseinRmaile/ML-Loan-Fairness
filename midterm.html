<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>ML 4641 Midterm Checkpoint - Spring 2025</title>
    <link rel="stylesheet" href="styles.css" />
    <script defer src="script.js"></script>
</head>
<body>

    <!-- Navigation Bar -->
    <nav class="navbar">
        <button id="btn-proposal" class="nav-button">Project Proposal</button>
        <button id="btn-midterm" class="nav-button">Midterm Checkpoint</button>
    </nav>

    <!-- Page Header -->
    <header>
        <h1>CS4641 Project Midterm Checkpoint</h1>
    </header>

    <main>
        <section class="section">
            <h2>BACKGROUND</h2>

            <h3>Introduction & Background</h3>
            <p>Algorithmic decision-making in the financial sector has become increasingly prevalent, particularly in credit risk assessment and loan approval processes. While these systems promise efficiency and scalability, they also raise concerns about fairness, transparency, and accountability—especially when models learn from historical data that may embed systemic biases. In high-stakes domains such as mortgage lending, unfair or biased predictions can have significant social consequences, reinforcing existing inequalities and denying opportunities to already marginalized groups.</p>

            <h3>Literature Review</h3>
            <p>Recent research has focused on quantifying and mitigating these algorithmic biases. Lee and Floridi [2] explore the ethical challenges of algorithmic fairness in mortgage lending, arguing that fairness metrics, while useful, cannot fully capture the complexities of discrimination. They emphasize the need for holistic approaches that address structural biases embedded within datasets. Their work highlights the difficult trade-offs between optimizing for accuracy and achieving fairness.</p>

            <p>Zhang et al. [3] propose adversarial training as a promising mitigation strategy. Their method involves a secondary model that penalizes unfair predictions, allowing the primary classifier to maintain its accuracy while reducing discriminatory behavior.</p>

            <p>In a complementary study, Bao et al. [4] demonstrate that hybrid models that integrate supervised and unsupervised learning outperform traditional models in predicting credit risk. They suggest that combining these predictive techniques with fairness-aware methods could lead to more balanced and equitable algorithms.</p>
        </section>

        <section class="section">
            <h2>DATASET DESCRIPTION</h2>
            <p>To explore the impact of fairness-aware machine learning in financial decision-making, we use the Snapshot National Loan-Level Dataset from the Home Mortgage Disclosure Act (HMDA) publication. This dataset provides comprehensive information on mortgage loan applications across the United States, including features such as applicant income, loan amount, credit score, loan type, property location, and demographic attributes like race, gender, and ethnicity. These features are essential not only for evaluating creditworthiness but also for assessing fairness and potential bias in loan approval decisions.</p>

            <p>We specifically selected data outside of the 2019–2021 period to avoid the distortions caused by the COVID-19 pandemic. During this time, mortgage lending practices were significantly impacted by emergency government policies, fluctuating interest rates, and unpredictable economic behavior. As a result, the patterns in loan approvals and denials may not reflect typical lending dynamics, which could skew model performance and fairness evaluations.</p>

            <p><strong>Dataset Link:</strong> <a href="https://ffiec.cfpb.gov/data-publication/snapshot-national-loan-level-dataset/2022" target="_blank">Snapshot National Loan-Level Dataset (2022)</a></p>
        </section>

        <section class="section">
            <h2>PROBLEM DEFINITION</h2>

            <h3>Problem</h3>
            <p>Machine learning models are increasingly used in financial lending to automate loan approval decisions. However, these models often inherit and amplify historical biases embedded in the data, disproportionately impacting marginalized communities such as racial minorities, immigrants, and older applicants. These biases manifest in higher rejection rates and less favorable loan terms—contributing to systemic financial exclusion. For instance, Latinx and Black borrowers collectively pay over $450 million more in interest annually compared to white borrowers [1].</p>

            <p>Even in competitive lending markets, bias persists due to institutional incentives that prioritize short-term profitability over long-term fairness. Traditional credit scoring systems further reinforce inequities by reflecting discriminatory practices of the past. Despite regulatory frameworks like the Equal Credit Opportunity Act (ECOA) and Fair Lending Laws, algorithmic lending systems continue to produce unfair outcomes. This undermines trust in automated financial tools and deepens socioeconomic disparities.</p>

            <p>Addressing these issues is essential. Our project aims to build fairness-aware loan approval models that retain predictive power while reducing bias. We incorporate fairness strategies throughout the machine learning pipeline—from data preprocessing and model selection to fairness metrics and evaluation. By doing so, we promote equitable, transparent, and responsible lending practices in the age of automation.</p>

            <h3>Motivation</h3>
            <p>Equitable access to credit is a cornerstone of financial inclusion and economic mobility. However, current lending models powered by machine learning often reinforce existing inequalities, especially when trained on biased historical data. This not only harms disadvantaged individuals but also exposes financial institutions to ethical, reputational, and regulatory risks.</p>

            <p>Our project is driven by the belief that predictive performance and fairness are not mutually exclusive. By developing interpretable and fairness-aware algorithms, we can ensure that lending decisions are not only accurate but also just. Through this approach, we seek to foster responsible AI adoption in finance, increase trust in algorithmic systems, and create meaningful pathways toward inclusive economic growth.</p>
        </section>

        <section class="section">
            <h2>METHODS</h2>

            <h3>Preprocessing</h3>
            <p>To increase data quality and maintain predictive accuracy, we began preprocessing by removing features with more than 50% null values. The dropped features included loan cost and fee-related fields such as <code>rate_spread</code>, <code>total_loan_costs</code>, <code>total_points_and_fees</code>, <code>origination_charges</code>, <code>discount_points</code>, <code>lender_credits</code>, <code>prepayment_penalty_term</code>, and <code>intro_rate_period</code>. The response variable, <code>action_taken</code>, was refined to represent loan approval or denial. Approved loans (1 – originated, 2 – approved but not accepted) were mapped to 1, while denied loans (3 – denied by the financial institution, 7 – preapproval request denied) were mapped to 0, with all other action codes dropped.</p>

            <p>For consistency, we focused on first-lien, 1–4 unit, owner-occupied, site-built, fully amortizing conventional loans intended for home purchase, home refinancing, and cash-out refinancing. We also consolidated co-applicant features by introducing a binary <code>has_co_applicant</code> flag, where 1 indicated the presence of a co-applicant and 0 indicated none. Other co-applicant demographic features, such as <code>co_applicant_credit_score_type</code>, <code>co_applicant_ethnicity_1</code>, <code>co_applicant_race_1</code>, and <code>co_applicant_sex</code>, were removed. Observed demographic features (<code>applicant_race_observed</code>, <code>applicant_ethnicity_observed</code>, <code>applicant_sex_observed</code>) were also dropped to mitigate potential bias.</p>

            <p>We addressed missing values in the <code>interest_rate</code> field by grouping rows by <code>debt_to_income_ratio</code> and computing the median interest rate within each group. Missing values were then imputed using the group median. Correlation analysis revealed weak positive relationships between features, and chi-square tests showed statistically significant relationships between approval rates and observed demographic features:</p>

            <ul>
                <li>Applicant ethnicity observed: χ² = 3936656.02, p = 0.0</li>
                <li>Applicant race observed: χ² = 3810281.38, p = 0.0</li>
            </ul>

            <div class="gantt-container">
                <img src="midterm images/loan approval rates by race observation status 1.png" alt="Approval rates by race observation status">
            </div>

            <div class="gantt-container">
                <img src="midterm images/loan approval rates by ethnicity observation status 1.png" alt="Approval rates by ethnicity observation status">
            </div>

            <div class="gantt-container">
                <img src="midterm images/loan approval rates by sex observation status 1.png" alt="Approval rates by sex observation status">
            </div>

            <p>There is a slight difference between approval rates when demographic data is observed rather than self-reported. To reduce bias, we excluded these rows.</p>

            <div class="gantt-container">
                <img src="midterm images/loan approval rates by sex 1.png" alt="Approval rates by sex">
            </div>

            <p>We remapped <code>conforming_loan_limits</code> to numerical values, converted <code>total_units</code> to integers, and removed loans marked as "Exempt" or "NA" for <code>debt_to_income_ratio</code>. Outlier handling and imputation steps were applied as follows:</p>

            <h3>Pre outlier income dist:</h3>
            <div class="gantt-container">
                <img src="midterm images/pre outlier income dist 1.png" alt="Income distribution before outlier handling">
            </div>

            <h3>After:</h3>
            <div class="gantt-container">
                <img src="midterm images/pre outlier income dist 2.png" alt="Income distribution after IQR method">
            </div>

            <h3>Pre outlier handling property value:</h3>
            <div class="gantt-container">
                <img src="midterm images/pre outlier handling property value 1.png" alt="Property value before outlier handling">
            </div>

            <h3>After:</h3>
            <div class="gantt-container">
                <img src="midterm images/pre outlier handling property value 2.png" alt="Property value after log transformation">
            </div>

            <h3>Pre loan amount outlier handling:</h3>
            <div class="gantt-container">
                <img src="midterm images/pre loan amount outlier handling 1.png" alt="Loan amount before outlier handling">
            </div>

            <h3>After:</h3>
            <div class="gantt-container">
                <img src="midterm images/pre loan amount outlier handling 2.png" alt="Loan amount after log transformation">
            </div>

            <p>We dropped features with correlation coefficient below 0.01 with <code>action_taken</code>, including <code>lei</code>, <code>purchaser_type</code>, <code>open_end_line_of_credit</code>, <code>submission_of_application</code>, <code>reverse_mortgage</code>, <code>business_or_commercial_purpose</code>, <code>initially_payable_to_institution</code>, <code>balloon_payment</code>, and <code>conforming_loan_limit</code>. <p>Feature engineering included deriving credit/employment history from denial reasons and estimating <code>monthly_payments</code> from loan amount, term, and interest rate.</p>

            <h3>ML Model Implemented</h3>
            <p>We implemented a Logistic Regression model, a classic supervised learning algorithm suitable for binary classification tasks. The goal was to predict whether a loan application would be approved or denied based on applicant, loan, and property-related features from the 2018 HMDA dataset. Missing values were addressed using context-specific imputations (e.g., income from FFIEC metro median income, interest rates by debt-to-income ratio grouping).</p>
            
            <p>Logistic Regression was selected due to its simplicity, interpretability, and efficiency when working with high-dimensional, tabular data. It serves as a strong baseline for feature signal evaluation and provides coefficients that indicate the relative importance of inputs. This approach aligns with supervised learning principles taught in class.</p>
            
            <h3>Justification</h3>
            <p>Logistic Regression was chosen as the primary model for this project because it strikes an ideal balance between interpretability, efficiency, and predictive power for structured, tabular data such as the HMDA loan application records. The goal of the project was not just to predict whether a loan would be approved or denied, but also to understand which features (e.g., income, loan amount, debt-to-income ratio) most influence that outcome.</p>
            
            <p>Logistic Regression is inherently interpretable — the model’s coefficients offer direct insights into the direction and strength of each feature’s impact. This is particularly important in the financial domain, where explainability is crucial for auditing and ethical considerations. Additionally, the binary nature of the target variable makes logistic regression a natural fit. The model performs well when features are linearly related to the log-odds of the outcome — a reasonable assumption after our preprocessing and feature engineering.</p>
            
            <p>Compared to more complex models, Logistic Regression is also less prone to overfitting on moderately sized datasets and allows for rapid iteration and validation. This made it a strategically sound and academically aligned choice for building a reliable, explainable baseline in the context of supervised learning and real-world decision modeling.</p>
            
        </section>

        <section class="section">
            <h2>RESULTS AND DISCUSSION</h2>

            <h3>Before dropping reverse_mortgage and open_end_line_of_credit:</h3>
            <ul>
                <li>Accuracy: 0.9460433664915614</li>
                <li>F1 Score: 0.9668908237343584</li>
                <li>ROC AUC Score: 0.9895407578939557</li>
            </ul>

            <p><strong>Classification Report:</strong></p>
            <pre>
precision    recall  f1-score   support

    0       0.75      0.99      0.85     28627
    1       1.00      0.94      0.97    150313

accuracy                           0.95    178940
macro avg       0.87      0.96      0.91    178940
weighted avg    0.96      0.95      0.95    178940
            </pre>

            <div class="gantt-container">
                <img src="midterm images/confusion matrix 1.png" alt="Confusion matrix before dropping features">
            </div>

            <div class="gantt-container">
                <img src="midterm images/ROC curve 1.png" alt="ROC curve before dropping features">
            </div>

            <h4>Fairness Metrics for 'race' (privileged = 1):</h4>
            <ul>
                <li>Demographic Parity Difference: -0.0109</li>
                <li>Disparate Impact Ratio: 0.9863</li>
            </ul>

            <h4>Fairness Metrics for 'applicant_sex' (privileged = 1):</h4>
            <ul>
                <li>Demographic Parity Difference: -0.0128</li>
                <li>Disparate Impact Ratio: 0.9839</li>
            </ul>

            <div class="gantt-container">
                <img src="midterm images/predicted loan acceptance by race 1.png" alt="Loan acceptance prediction by race (before)">
            </div>

            <h3>After dropping reverse_mortgage and open_end_line_of_credit:</h3>
            <ul>
                <li>Accuracy: 0.7260198949368504</li>
                <li>F1 Score: 0.817997549838512</li>
                <li>ROC AUC Score: 0.7881808178848329</li>
            </ul>

            <p><strong>Classification Report:</strong></p>
            <pre>
precision    recall  f1-score   support

    0       0.33      0.69      0.45     28627
    1       0.93      0.73      0.82    150313

accuracy                           0.73    178940
macro avg       0.63      0.71      0.63    178940
weighted avg    0.83      0.73      0.76    178940
            </pre>

            <div class="gantt-container">
                <img src="midterm images/confusion matrix 2.png" alt="Confusion matrix after dropping features">
            </div>

            <div class="gantt-container">
                <img src="midterm images/ROC curve 2.png" alt="ROC curve after dropping features">
            </div>

            <h4>Fairness Metrics for 'race' (privileged = 1):</h4>
            <ul>
                <li>Demographic Parity Difference: -0.3400</li>
                <li>Disparate Impact Ratio: 0.6299</li>
            </ul>

            <h4>Fairness Metrics for 'applicant_sex' (privileged = 1):</h4>
            <ul>
                <li>Demographic Parity Difference: -0.1611</li>
                <li>Disparate Impact Ratio: 0.7820</li>
            </ul>

            <div class="gantt-container">
                <img src="midterm images/predicted loan acceptance by race 2.png" alt="Loan acceptance prediction by race (after)">
            </div>

            <h3>Results and Performance Discussion</h3>
            <p>Our Logistic Regression model achieved an accuracy of 72.6%, doing a moderate job of predicting loan approvals and denials, with some room for improvement. The F1-score of 81.8% suggests that the model balances between precision and recall.</p>

            <p>The model struggles with denied loans (class 0) as it correctly identifies them only 33% of the time (precision), though it catches 69% of actual denials (recall). Therefore it is prone to incorrectly approving applications that should have been denied. On the other hand, for approved loans (class 1), it’s more confident: 93% precision means most approvals are correctly classified, but the 73% recall shows that it still misses some valid approvals.</p>

            <p>The ROC AUC score of 0.79 tells us that while the model is better than random guessing, it still struggles to cleanly separate approvals from denials. Beyond accuracy, fairness is another major concern. Looking at the fairness metrics, we see significant disparities across demographic groups. Loan approvals are less likely for non-privileged racial groups (Disparate Impact Ratio = 0.63, below the 0.8 fairness threshold). Gender disparities also exist (DIR = 0.78), with lower approval rates for one gender. The Demographic Parity Differences (-0.34 for race, -0.16 for sex) confirm unequal treatment.</p>

            <p>Overall, while the model performs well in identifying approvals, it struggles with fairness and denied loan predictions. Visual tools like ROC curves, confusion matrices, and fairness comparisons help clarify where improvement is needed.</p>

            <p>Our model performed well largely because mortgage lending decisions follow clear-cut financial rules that are easy for a classification model to learn. The mortgage industry relies on well-defined underwriting criteria, such as credit scores, income, debt-to-income ratio, and loan-to-value ratio, which are strong, structured predictors of loan approval. Since these financial factors have a direct impact on lending decisions, they provide a clear decision-making framework that aligns well with a model like Logistic Regression. Additionally, we conducted extensive data preprocessing to clean and structure the dataset, including handling missing values, engineering new features, and standardizing key financial metrics. With a large and structured dataset (2018 HMDA dataset, nearly 180,000 applications), the model had ample examples to learn patterns in past approvals and denials, allowing it to perform well overall despite some limitations in correctly identifying denials and fairness disparities.</p>
        
        </section>

        <section class="section">
            <h2>NEXT STEPS</h2>
            <p>Based on how sensitive the model performance and fairness were to feature selection, our next steps will focus on refining both our preprocessing and evaluation approaches.</p>

            <ul>
                <li><strong>Reassess Feature Selection:</strong> We'll take a closer look at the features we removed and run experiments to see how they impact the model—both on their own and in combination with other variables. Instead of only using correlation thresholds, we’ll try model-driven methods like recursive feature elimination and permutation importance to see which features matter most.</li>
                <li><strong>Restore Key Features:</strong> The performance drop after removing <code>reverse_mortgage</code> and <code>open_end_line_of_credit</code> shows these features likely provide important information. We’ll bring these features back into the model and check if doing so improves both predictive accuracy and fairness.</li>
                <li><strong>Expand Fairness Analysis:</strong> While we’ll keep evaluating demographic parity, we’ll add other fairness metrics such as equal opportunity difference and average odds difference to get a more comprehensive understanding of any biases in the model.</li>
                <li><strong>Test Fairness Mitigation Techniques:</strong> We plan to evaluate methods like reweighing and adversarial debiasing to reduce bias without severely harming performance.</li>
                <li><strong>Introduce Additional Models:</strong> To establish stronger baselines, we plan to experiment with ensemble methods like Random Forests and Gradient Boosting. These models might offer better performance while maintaining fairness.</li>
                <li><strong>Evaluate Unsupervised Learning:</strong> In the next phase, we’ll explore unsupervised learning methods like clustering to uncover hidden patterns in the data. This can guide fairness-aware preprocessing by highlighting groups that may not be reflected in demographic features but still impact loan decisions.</li>
            </ul>
            

            <p>These steps will help us create a more balanced model pipeline—one that maintains predictive accuracy while also addressing fairness concerns.</p>
        </section>

        <section class="section">
            <h2>GANTT CHART</h2>
            <div class="gantt-container">
                <img src="midterm images/ML spring 2025 midterm checkpoint plan.png" alt="Midterm Gantt Chart">
            </div>
        </section>

        <section class="section">
            <h2>TEAM CONTRIBUTIONS</h2>

            <h3>Aarthi</h3>
            <ul>
                <li>Wrote the introduction, background, dataset overview, problem definition, and project motivation.</li>
                <li>Developed the Logistic Regression evaluation model.</li>
                <li>Wrote the ML model selection section and added clarity to preprocessing/methodology.</li>
                <li>Provided support and feedback for other sections and structured the final layout.</li>
            </ul>

            <h3>Suhana</h3>
            <ul>
                <li>Led data preprocessing, including missing value handling and feature engineering.</li>
                <li>Developed performance/fairness visualizations.</li>
                <li>Implemented and evaluated Logistic Regression.</li>
                <li>Collaborated across all sections to ensure consistency.</li>
            </ul>

            <h3>Divya</h3>
            <ul>
                <li>Handled preprocessing, dimensionality reduction, and standardization.</li>
                <li>Wrote the Results, Discussion, and References sections.</li>
                <li>Analyzed fairness and performance metrics.</li>
            </ul>

            <h3>Hussein</h3>
            <ul>
                <li>Debugged preprocessing steps and helped engineer features.</li>
                <li>Helped build and evaluate Logistic Regression model.</li>
                <li>Wrote first draft of Results and Next Steps.</li>
                <li>Updated the Gantt chart and GitHub Pages navigation.</li>
            </ul>

            <h3>Trisha</h3>
            <ul>
                <li>Worked on data cleaning, feature engineering, and model training.</li>
                <li>Implemented dimensionality reduction techniques.</li>
                <li>Evaluated model performance and supported team writing/editing.</li>
            </ul>
        </section>

        <section class="section">
            <h2>REFERENCES</h2>
            <ol>
                <li>R. Bartlett, A. Morse, R. Stanton, and N. Wallace, “Consumer-lending discrimination in the FinTech Era,” <em>Journal of Financial Economics</em>, vol. 143, no. 1, pp. 30–56, 2022. <a href="https://doi.org/10.1016/j.jfineco.2021.05.047" target="_blank">Link</a></li>
                <li>M.S.A. Lee and L. Floridi, “Algorithmic Fairness in Mortgage Lending,” <em>Minds & Machines</em>, vol. 31, pp. 165–191, 2021. <a href="https://doi.org/10.1007/s11023-020-09529-4" target="_blank">Link</a></li>
                <li>B. Zhang, B. Lemoine, and M. Mitchell, “Mitigating Unwanted Biases with Adversarial Learning,” <em>Proc. of AIES '18</em>, pp. 335–340, 2018. <a href="https://doi.org/10.1145/3278721.3278779" target="_blank">Link</a></li>
                <li>W. Bao, N. Lianju, and K. Yua, “Integration of unsupervised and supervised ML algorithms for credit risk assessment,” <em>Expert Systems with Applications</em>, vol. 128, pp. 301–315, 2019. <a href="https://doi.org/10.1016/j.eswa.2019.02.033" target="_blank">Link</a></li>
                <li>“Factor analysis of mixed data,” Prince library, accessed Mar. 31, 2025. <a href="https://maxhalford.github.io/prince/famd/" target="_blank">Link</a></li>
            </ol>
        </section>
    </main>
</body>
</html>